import pandas as pd
import numpy as np
import glob
import os
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# ==========================================
# 1. 設定・マッピング定義
# ==========================================

NORMAL_DIR = r"D:\CSV_Flow\CSV_Normal\*.csv"
ATTACK_DIR = r"D:\CSV_Flow\CSV_Attacked\*.csv"

# サンプリング率（攻撃データは非常に多いため、さらに絞ります）
RATIO_NORMAL = 1.0     # 正常は全件
RATIO_ATTACK = 0.005   # 攻撃は0.5% (多値分類なので少し多めに確保)


MALWARE_MAPPING = {
    'normal': 'Normal',
    
    # ★ここを追加・強化！
    '192.168.100.103': 'Kenjiro',
    '192.168.100.113': 'Gafgyt',
    '192.168.100.111': 'Mirai',     # Scenario 14, 15
    '192.168.100.108': 'Okiru',     # Scenario 11 (Otherに入っている可能性大)
    '192.168.100.147': 'Hajime',    # Scenario 18
    '192.168.100.148': 'Hajime', 
    '192.168.100.149': 'Hajime',
    '192.168.100.150': 'Hajime',
    '192.168.1.1': 'Mirai',         # Scenario 1 (初期のMirai)
    
    # 一般的な名称
    'Hajime': 'Hajime',
    'Muhstik': 'Muhstik',
    'Torii': 'Torii',
    'Trojan': 'Trojan',
    'IRCBot': 'IRCBot',
    'Hide': 'HideAndSeek',
    'Gafgyt': 'Gafgyt',
    'Mirai': 'Mirai',
    'Kenjiro': 'Kenjiro',
    'Okiru': 'Okiru'
}

DROP_COLUMNS = ['flow_id', 'src_ip', 'dst_ip', 'src_port', 'dst_port', 'timestamp', 'label']

# ==========================================
# 2. 関数定義
# ==========================================

def get_label_from_filename(filename):
    """ ファイル名からマルウェア名を判定する関数 """
    base_name = os.path.basename(filename)
    
    # マッピング辞書を走査して、該当する文字列が含まれていればそのラベルを返す
    for keyword, label in MALWARE_MAPPING.items():
        if keyword in base_name:
            return label
    
    # 正常フォルダにあるのに名前が一致しない場合
    if "Normal" in filename:
        return "Normal"
        
    # どのキーワードにも当てはまらない場合
    return "Other_Malware"

def load_and_reduce_files(file_list, sample_ratio):
    df_list = []
    
    for i, filename in enumerate(file_list):
        try:
            df = pd.read_csv(filename)
            
            # 不要列削除
            cols = [c for c in DROP_COLUMNS if c in df.columns]
            df.drop(columns=cols, inplace=True)
            
            # サンプリング
            if sample_ratio < 1.0:
                df = df.sample(frac=sample_ratio, random_state=42)
            
            # 型変換
            float_cols = df.select_dtypes(include=['float64']).columns
            df[float_cols] = df[float_cols].astype('float32')
            
            # ★ここで多値ラベルを付与
            label_name = get_label_from_filename(filename)
            df['label'] = label_name
            
            df_list.append(df)
            
            if (i + 1) % 5 == 0:
                print(f"  [{i+1}/{len(file_list)}] {label_name}: {os.path.basename(filename)}")
                
        except Exception as e:
            print(f"  Error: {filename} -> {e}")

    if not df_list:
        return pd.DataFrame()
    
    return pd.concat(df_list, ignore_index=True)

def preprocess_data(df):
    if df.empty: return None, None
    df = df.sample(frac=1, random_state=42).reset_index(drop=True)
    y = df['label'] # 文字列のラベル (Normal, Mirai, ...)
    X = df.drop(columns=['label'])
    X = X.replace([np.inf, -np.inf], np.nan).fillna(0)
    return X, y

# ==========================================
# 3. メイン処理 (修正版)
# ==========================================

def main():
    print("=== 多値分類処理を開始（修正版） ===")
    
    # 1. 全ファイルリストを取得（まだ分割しない！）
    all_normal_files = glob.glob(NORMAL_DIR)
    all_attack_files = glob.glob(ATTACK_DIR)
    
    print(f"正常ファイル数: {len(all_normal_files)}")
    print(f"攻撃ファイル数: {len(all_attack_files)}")

    # 2. データを読み込んで1つにまとめる
    #    ここで全てのファイルから少しずつデータを集めます
    print("\n[Step 1] 全データの読み込み & サンプリング")
    
    # 正常データ (100%使用)
    df_normal = load_and_reduce_files(all_normal_files, RATIO_NORMAL)
    
    # 攻撃データ (0.5%使用)
    df_attack = load_and_reduce_files(all_attack_files, RATIO_ATTACK)
    
    # 結合して1つの巨大なデータセットにする
    df_full = pd.concat([df_normal, df_attack], ignore_index=True)
    
    # メモリ解放
    del df_normal, df_attack
    import gc
    gc.collect()
    
    # データ内訳の確認
    print("\n[データ内訳 (分割前)]")
    print(df_full['label'].value_counts())
    
    if len(df_full) == 0:
        print("エラー: データが読み込めませんでした。")
        return

    # 3. 前処理 (Xとyに分離)
    print("\n[Step 2] 前処理")
    X, y = preprocess_data(df_full)
    
    # メモリ解放
    del df_full
    gc.collect()

    # 4. ここで初めて学習用とテスト用に分割する (Hold-out法)
    #    stratify=y を指定することで、ラベルの比率を保ったまま分割してくれます
    print("\n[Step 3] データ分割 (Train: 70%, Test: 30%)")
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )

    # 5. 学習
    print(f"\n[Step 4] 学習 (Train: {len(X_train)}件)")
    rf_model = RandomForestClassifier(
        n_estimators=100, 
        class_weight='balanced', 
        random_state=42, 
        n_jobs=-1
    )
    rf_model.fit(X_train, y_train)
    
    # 6. 評価
    print(f"\n[Step 5] 評価 (Test: {len(X_test)}件)")
    y_pred = rf_model.predict(X_test)
    
    print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
    
    print("\n--- Classification Report ---")
    # zero_division=0 を指定して、万が一データがない場合も警告で止まらないようにする
    print(classification_report(y_test, y_pred, zero_division=0))
    
    # 混同行列
    labels = sorted(y_test.unique())
    cm = confusion_matrix(y_test, y_pred, labels=labels)
    
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title('Confusion Matrix (Multiclass)')
    plt.show()

if __name__ == "__main__":
    main()